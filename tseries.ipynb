{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from iris import irisRequests\n",
    "import datetime\n",
    "from numba import njit\n",
    "from numba.typed import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn((10,20)).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = {}\n",
    "regions['greece'] = (30, 45,18, 44)\n",
    "regions['california'] = (30, 41, -125, -113)\n",
    "regions['japan'] = (20, 50, 120, 150)\n",
    "regions['italy'] = (35,46,6, 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincenzo/Desktop/quake_utils/iris.py:202: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(download_url, sep=\"|\", comment=\"#\")\n",
      "/Users/vincenzo/Desktop/quake_utils/iris.py:202: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(download_url, sep=\"|\", comment=\"#\")\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime(2003, 1, 1, 0, 0, 0)\n",
    "end_time =  datetime.datetime(2024, 1, 1, 0, 0, 0)\n",
    "catalogs = {}\n",
    "for name, region in regions.items():\n",
    "    df  =irisRequests.retrieve_events_box(start_time, end_time, region[0], region[1], region[2], region[3], minmag=3, magtype=\"MW\")\n",
    "    df.Time = df.Time.dt.tz_localize(None)\n",
    "    catalogs[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(nogil=True)\n",
    "def split_times(times, origin_shift, t_end ):\n",
    "    # find first index to match origin_shift\n",
    "    origin_i = 0\n",
    "    for i in range(0, len(times)):\n",
    "        if(times[i] >= origin_shift):\n",
    "            origin_i = i\n",
    "            break\n",
    "    begin_indices = []\n",
    "    end_indices = []\n",
    "    while(origin_i < len(times)):\n",
    "        begin_indices.append(origin_i)\n",
    "        end_i = origin_i + 1\n",
    "        while(times[end_i] - times[origin_i] <= t_end):\n",
    "            end_i += 1\n",
    "            if(end_i >= len(times)):\n",
    "                break\n",
    "        end_indices.append(end_i)\n",
    "        \n",
    "        origin_i = end_i\n",
    "    return begin_indices, end_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "times, features = catalogs['japan'].Time.values, catalogs['japan'][['Latitude', 'Longitude', 'Magnitude']]\n",
    "times = times - times[0]\n",
    "times = times.astype('timedelta64[s]').astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 5.62800000e+03, 7.97100000e+03, ...,\n",
       "       6.55432316e+08, 6.55440602e+08, 6.55633038e+08])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNPP(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features : int, emb_dim : int, dropout : float, layer_norm : bool):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(emb_dim) if layer_norm else None\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.linear(x))\n",
    "        if(self.norm is not None):\n",
    "            return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weibull:\n",
    "    def __init__(self, N, k, eps=1e-8):\n",
    "        # b and k are strictly positive tensors of the same shape\n",
    "        self.b = b\n",
    "        self.k = k\n",
    "        self.eps = eps\n",
    "    \n",
    "    def log_prob(self, x):\n",
    "        \"\"\"Logarithm of the probability density function log(f(x)).\"\"\"\n",
    "        # x must have the same shape as self.b and self.k\n",
    "        x = x.clamp_min(self.eps)  # pow is unstable for inputs close to 0\n",
    "        return (self.b.log() + self.k.log() + (self.k - 1) * x.log() \n",
    "                + self.b.neg() * torch.pow(x, self.k))\n",
    "    \n",
    "    def log_survival(self, x):\n",
    "        \"\"\"Logarithm of the survival function log(S(x)).\"\"\"\n",
    "        x = x.clamp_min(self.eps)\n",
    "        return self.b.neg() * torch.pow(x, self.k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exponential:\n",
    "    def __init__(self, b):\n",
    "        self.b = b\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        return self.b.log() + self.b.neg()*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveNPP(nn.Module):\n",
    "    def __init__(self, in_features : int, emb_dim : int, num_heads : int, dropout : float, batch_first : bool, hidden_dim : int):\n",
    "        super().__init__()\n",
    "        assert emb_dim % num_heads == 0\n",
    "        self.batch_first = batch_first\n",
    "        self.emb =EmbeddingNPP(in_features, emb_dim, dropout, True) #  apply everywhere the embedding\n",
    "        self.mha = nn.MultiheadAttention(emb_dim, num_heads,\n",
    "                                           dropout,\n",
    "                                           batch_first=batch_first,\n",
    "                                           kdim=emb_dim,\n",
    "                                           vdim=emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim,hidden_dim,\n",
    "                          num_layers=1,\n",
    "                          batch_first=batch_first)\n",
    "        self.gr_mod = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 1), \n",
    "            nn.Softplus()) # GR parameter\n",
    "        self.weibull_mod = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 2), nn.Softplus()) # Waiting time parameters\n",
    "\n",
    "    def forward(self,z,inter_times): # z contains all the other info (mag, lat, lon, depth) for example\n",
    "        # inter_times (N, L)\n",
    "        # z (N, L, K) \n",
    "        x = torch.cat([z, inter_times.unsqueeze(-1)], dim=-1)  \n",
    "        x = self.emb(x)\n",
    "        attn_output, _ = self.mha(x, x, x) # no need the weights, atm\n",
    "        rnn_output, _ = self.rnn(attn_output)\n",
    "        # shift forward along the time dimension and pad\n",
    "        context = F.pad(rnn_output[:, :-1, :], (0,0, 1,0))\n",
    "        return context\n",
    "\n",
    "    def get_time_nll(self, inter_times, context, seq_lengths):\n",
    "        weibull_params = self.weibull_mod(context)\n",
    "        weibull = Weibull(weibull_params[...,0], weibull_params[...,1])\n",
    "        log_pdf = weibull.log_prob(inter_times)\n",
    "        arange = torch.arange(inter_times.shape[1], device=seq_lengths.device)\n",
    "        mask = (arange[None, :] < seq_lengths[:, None]).float()  # (N, L)\n",
    "        log_like = (log_pdf * mask).sum(-1)  # (N,)\n",
    "        log_surv = weibull.log_survival(inter_times)  # (N, L)\n",
    "        end_idx = seq_lengths.unsqueeze(-1)  # (N, 1)\n",
    "        log_surv_last = torch.gather(log_surv, dim=-1, index=end_idx)  # (N, 1)\n",
    "        log_like += log_surv_last.squeeze(-1)  # (N,)\n",
    "        return -log_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentiveNPP(4,64,4, 0.2, True, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_times = torch.zeros((10,100)).exponential_()\n",
    "context = model(torch.randn((10, 100, 3 )), inter_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2121, -0.5218, -0.6996, -1.1791, -2.0170, -0.8205, -0.5913, -0.7628,\n",
       "         -1.0624, -0.4477, -1.1397, -1.1023, -1.3869, -0.8901, -1.4265, -0.9131,\n",
       "         -0.5053, -0.7892, -0.4738, -0.5507, -0.5950, -0.4919, -1.7662, -0.6275,\n",
       "         -2.3188, -0.6468, -1.4172, -0.5552, -0.8088, -0.6932, -1.6195, -1.0932,\n",
       "         -0.6750, -0.7479, -0.4915, -0.9567, -0.6205, -0.6935, -0.5089, -1.4511,\n",
       "         -0.8642, -0.5096, -1.0778, -1.2879, -0.7289, -0.5897, -1.3612, -0.6941,\n",
       "         -1.4936, -1.3143, -1.0286, -0.7985, -1.7222, -3.8011, -0.5063, -0.6236,\n",
       "         -1.3824, -0.9613, -0.5173, -1.2240, -0.6452, -1.7097, -0.6350, -0.8988,\n",
       "         -0.5568, -0.6035, -0.6933, -1.0979, -1.9476, -0.7162, -1.4590, -0.8381,\n",
       "         -0.6385, -1.0851, -0.8592, -1.4093, -1.3474, -2.6896, -1.8480, -0.4968,\n",
       "         -0.5954, -0.7744, -0.5878, -0.7064, -1.6996, -0.7946, -2.7677, -1.1159,\n",
       "         -1.2443, -0.5470, -1.4339, -0.7899, -0.9116, -0.8763, -0.7571, -1.0739,\n",
       "         -0.6575, -1.9952, -1.6146, -1.3356],\n",
       "        [-0.9195, -0.4711, -0.6483, -1.6187, -2.1032, -1.4301, -2.5832, -0.9733,\n",
       "         -2.1506, -2.0001, -0.7767, -1.0132, -0.7552, -0.7657, -0.5043, -1.3500,\n",
       "         -0.5556, -1.7344, -1.0973, -0.4713, -1.1804, -0.7898, -1.6133, -0.6390,\n",
       "         -1.3006, -0.9181, -0.8910, -1.4942, -2.6766, -1.4349, -1.1633, -0.5768,\n",
       "         -1.0223, -0.7108, -0.7785, -0.6900, -2.0209, -2.2503, -1.1407, -1.2036,\n",
       "         -1.4139, -0.7746, -1.4306, -0.8286, -1.5936, -0.6056, -2.0919, -1.2210,\n",
       "         -0.9733, -1.7834, -0.9175, -0.6151, -0.6335, -0.7263, -0.7929, -0.9221,\n",
       "         -0.7848, -0.8346, -0.7844, -0.9473, -0.9417, -1.2239, -0.5305, -1.5457,\n",
       "         -1.0084, -0.6528, -0.9370, -0.7617, -1.7123, -0.9768, -0.9812, -0.9383,\n",
       "         -1.3535, -1.5141, -0.6614, -1.1583, -0.9673, -0.9131, -1.9815, -0.8012,\n",
       "         -1.1769, -0.7713, -0.5135, -0.5696, -0.4650, -0.6966, -0.4882, -0.8364,\n",
       "         -0.5322, -0.6827, -0.5587, -1.8709, -0.7489, -1.6321, -0.5004, -0.9399,\n",
       "         -0.6148, -0.8828, -1.1307, -1.2896],\n",
       "        [-1.1719, -1.5336, -0.5999, -0.6842, -0.8362, -1.8998, -0.5214, -0.8377,\n",
       "         -0.6533, -1.0652, -1.1323, -1.0031, -3.5356, -0.9628, -1.7299, -1.5891,\n",
       "         -1.1593, -0.6834, -1.0619, -1.1402, -0.5938, -0.6199, -1.9742, -0.8111,\n",
       "         -1.1974, -0.9017, -0.9537, -0.8749, -0.6263, -1.5922, -0.8979, -0.5726,\n",
       "         -0.6659, -1.1424, -1.8886, -0.5281, -3.9663, -1.1736, -0.8431, -2.7759,\n",
       "         -1.3315, -1.3936, -0.5741, -1.0691, -0.7608, -0.8920, -0.6927, -1.5164,\n",
       "         -0.4641, -0.4796, -2.2448, -0.8999, -0.4616, -0.5417, -0.8823, -0.6285,\n",
       "         -0.7120, -0.5100, -1.1320, -1.7722, -0.8649, -0.5605, -1.0466, -1.4294,\n",
       "         -0.5361, -0.7362, -0.8040, -0.5869, -0.8132, -1.4084, -1.0386, -1.6633,\n",
       "         -1.4658, -1.1146, -1.3695, -1.2526, -0.8159, -1.1948, -0.8326, -1.9978,\n",
       "         -0.7691, -0.9948, -0.6017, -0.8774, -2.0298, -0.7065, -0.5603, -0.8576,\n",
       "         -1.7405, -0.5396, -0.6111, -0.6418, -0.5652, -0.8718, -0.5104, -0.5407,\n",
       "         -2.1143, -1.0280, -1.0162, -1.9866],\n",
       "        [-0.9821, -1.6507, -1.7787, -0.4593, -0.8775, -0.7257, -1.7704, -0.6089,\n",
       "         -0.7740, -1.7913, -0.9849, -2.1023, -1.2330, -2.0021, -1.4760, -0.5771,\n",
       "         -1.2244, -1.4486, -2.2301, -2.0050, -0.6568, -0.4960, -1.4971, -0.5938,\n",
       "         -0.4899, -1.2023, -0.4532, -1.6232, -0.6417, -0.6186, -1.2007, -1.2385,\n",
       "         -0.4845, -0.5071, -1.1197, -1.0871, -0.6420, -1.0005, -0.8949, -0.5012,\n",
       "         -1.3590, -3.3086, -0.5134, -0.9363, -1.0142, -0.5368, -0.4710, -0.6161,\n",
       "         -1.1165, -0.5954, -0.4871, -0.4732, -3.4043, -0.4950, -2.6299, -0.6750,\n",
       "         -0.8292, -1.8351, -0.6810, -0.8220, -1.0673, -0.5046, -1.8818, -1.1234,\n",
       "         -0.9573, -0.9714, -0.6934, -0.6013, -0.7017, -1.0873, -0.9702, -0.5990,\n",
       "         -0.5816, -2.2562, -1.5019, -0.4949, -0.6745, -0.7236, -0.8631, -1.8808,\n",
       "         -0.5511, -2.1765, -0.7527, -1.5643, -0.7933, -1.4029, -0.9148, -1.4537,\n",
       "         -0.5409, -0.6711, -0.8475, -1.6213, -0.5493, -1.5166, -0.8982, -1.0692,\n",
       "         -0.8471, -1.1407, -1.6970, -1.0944],\n",
       "        [-0.7736, -2.1887, -1.2730, -1.1997, -0.8267, -0.7774, -1.1245, -0.8011,\n",
       "         -0.5833, -0.8992, -0.5950, -0.8372, -0.7066, -1.2296, -0.5908, -0.9326,\n",
       "         -2.0438, -1.1940, -1.2324, -0.5728, -1.1414, -3.3768, -0.7514, -0.4852,\n",
       "         -1.6385, -1.4386, -2.5465, -0.4909, -0.6499, -1.0870, -0.5405, -0.5187,\n",
       "         -0.5984, -0.9443, -1.3088, -0.5224, -0.9395, -2.9678, -0.6268, -0.4730,\n",
       "         -2.0964, -1.2862, -4.8325, -1.1980, -0.7706, -1.9043, -0.9333, -0.8752,\n",
       "         -3.0418, -1.3764, -1.4669, -0.6164, -0.7066, -0.8038, -0.8817, -0.5824,\n",
       "         -1.9359, -0.5056, -1.1871, -1.5685, -0.9633, -2.3252, -1.6228, -0.8912,\n",
       "         -1.5554, -1.0472, -0.9719, -0.5214, -1.7554, -1.3617, -1.0574, -1.2113,\n",
       "         -0.7730, -1.4625, -2.8092, -0.7788, -0.5885, -0.4828, -0.8316, -1.9459,\n",
       "         -1.4666, -0.5767, -1.5259, -2.1048, -1.5181, -2.8911, -1.1595, -0.5760,\n",
       "         -0.6706, -1.5417, -1.3982, -1.4261, -0.6055, -0.4669, -0.7486, -1.6241,\n",
       "         -1.5344, -0.4720, -0.7478, -0.5399],\n",
       "        [-1.6489, -1.4455, -2.6786, -0.9301, -1.0214, -1.0312, -1.6393, -0.5033,\n",
       "         -0.4862, -1.1786, -1.4275, -0.8076, -0.8218, -0.4734, -0.9487, -1.9385,\n",
       "         -1.6865, -0.5725, -1.3163, -0.9045, -0.5925, -1.1173, -0.9049, -1.1590,\n",
       "         -1.8679, -1.7593, -0.9135, -1.0696, -0.5530, -0.7410, -2.4154, -1.7323,\n",
       "         -1.0998, -2.4087, -0.6638, -0.4605, -0.5274, -0.4915, -0.5290, -0.6860,\n",
       "         -0.8146, -0.7232, -0.9344, -0.7063, -0.8031, -0.5909, -1.5442, -0.4669,\n",
       "         -0.4486, -0.6702, -1.2816, -1.0595, -1.7181, -0.4978, -0.4907, -1.9221,\n",
       "         -1.7338, -0.8572, -1.2442, -0.6196, -0.5766, -0.9898, -1.7716, -1.5613,\n",
       "         -1.2345, -0.5980, -1.5754, -0.9031, -2.9312, -1.8245, -0.8748, -0.4445,\n",
       "         -1.0885, -0.6631, -0.7726, -1.3204, -0.7546, -1.0840, -0.8140, -0.6929,\n",
       "         -1.5609, -1.5379, -0.9596, -0.5216, -0.9773, -0.9169, -0.5812, -0.7097,\n",
       "         -1.0303, -1.3798, -0.5205, -0.6206, -1.0631, -0.5943, -1.2657, -2.0447,\n",
       "         -0.5936, -1.7698, -0.6822, -0.5124],\n",
       "        [-3.0822, -0.7319, -1.6879, -1.0009, -1.0142, -0.6405, -0.9222, -0.5954,\n",
       "         -0.7315, -1.0877, -0.6998, -1.3130, -0.7947, -0.7916, -0.9270, -0.7993,\n",
       "         -0.7890, -0.6886, -0.5695, -0.6210, -1.5039, -2.7534, -0.8924, -2.2907,\n",
       "         -1.2975, -0.7304, -0.9834, -1.3371, -0.4581, -0.4802, -0.9927, -1.3083,\n",
       "         -2.3014, -0.6544, -0.6194, -2.7619, -1.4599, -1.3901, -1.0176, -1.3749,\n",
       "         -0.7765, -0.4636, -0.8680, -0.5977, -1.2816, -0.8923, -1.1923, -1.8381,\n",
       "         -0.7373, -2.6844, -1.1466, -0.5511, -1.5864, -0.6082, -0.7242, -0.6743,\n",
       "         -0.7803, -1.3793, -0.6246, -1.5352, -0.9818, -0.9046, -1.2938, -0.4947,\n",
       "         -0.5168, -2.7814, -0.6308, -1.5424, -0.9926, -0.7917, -0.4742, -0.8468,\n",
       "         -0.9208, -0.5853, -0.9438, -0.9048, -0.9956, -1.3336, -1.9574, -0.6811,\n",
       "         -0.5173, -0.8100, -1.5045, -0.9975, -2.1361, -0.5137, -0.9271, -0.4739,\n",
       "         -0.9093, -1.6519, -1.7735, -0.6930, -1.3746, -0.6970, -0.4626, -1.6396,\n",
       "         -0.7000, -0.8768, -1.8034, -0.5333],\n",
       "        [-0.4700, -0.7802, -0.6882, -2.1058, -1.2650, -0.6966, -0.5724, -0.9074,\n",
       "         -0.6514, -1.1233, -1.2567, -0.9460, -2.6815, -1.3492, -1.1548, -1.6491,\n",
       "         -0.5271, -1.3783, -1.4050, -0.9676, -1.1081, -0.8714, -1.3195, -1.5199,\n",
       "         -0.8789, -0.9515, -1.0485, -0.5946, -0.7571, -0.4796, -1.1303, -0.7961,\n",
       "         -0.6665, -0.6478, -0.8208, -2.2635, -0.7517, -1.3402, -0.4727, -0.7949,\n",
       "         -0.8388, -0.6543, -0.5813, -0.6958, -1.9645, -0.4512, -1.1820, -0.7774,\n",
       "         -0.4457, -1.4101, -1.0240, -3.1968, -0.9692, -1.1934, -0.9987, -1.6996,\n",
       "         -0.5447, -0.9394, -1.4612, -0.6216, -0.5773, -1.0325, -1.1173, -0.9631,\n",
       "         -0.5337, -0.6900, -0.6191, -0.9762, -2.6995, -0.9206, -0.9457, -0.7781,\n",
       "         -0.7032, -0.8288, -1.8068, -0.7369, -0.6869, -0.4353, -0.7315, -0.4288,\n",
       "         -2.0674, -1.7338, -1.8803, -0.5700, -0.8886, -0.6526, -1.1920, -0.4780,\n",
       "         -0.5395, -1.3610, -0.9267, -0.7823, -0.7218, -1.0822, -1.8713, -1.9593,\n",
       "         -0.5680, -1.1581, -0.8855, -0.6993],\n",
       "        [-1.1562, -1.2794, -2.1259, -0.8865, -0.7811, -0.6815, -0.8134, -2.4697,\n",
       "         -1.6504, -0.8487, -0.9409, -1.3377, -0.7552, -1.0512, -0.5038, -0.4504,\n",
       "         -0.5139, -0.4353, -1.2184, -1.0940, -1.5317, -0.7652, -1.2384, -0.5828,\n",
       "         -0.6693, -0.6877, -0.5088, -0.9558, -0.5459, -0.7174, -1.2681, -2.5454,\n",
       "         -1.1750, -0.5395, -2.6675, -2.7281, -0.4646, -0.6102, -1.5935, -0.7855,\n",
       "         -2.3803, -1.4527, -1.3081, -0.9733, -2.3381, -2.3932, -0.7493, -0.5406,\n",
       "         -0.7544, -1.8904, -0.8655, -1.5338, -0.9972, -0.9585, -1.5798, -0.8781,\n",
       "         -1.6247, -0.5875, -0.4687, -1.0391, -2.0511, -0.5978, -0.8362, -0.9881,\n",
       "         -0.6858, -0.6196, -1.8281, -0.8736, -0.4508, -0.9050, -0.7644, -0.9369,\n",
       "         -1.8961, -1.8497, -1.2425, -0.8960, -1.3407, -0.7112, -1.3277, -0.8790,\n",
       "         -0.5363, -1.2926, -0.7410, -0.7566, -0.4812, -1.6935, -1.1617, -2.2110,\n",
       "         -0.9746, -0.9498, -1.7669, -0.9210, -0.5006, -1.0389, -0.9791, -0.6594,\n",
       "         -0.6985, -0.9835, -1.4408, -0.6794],\n",
       "        [-1.0667, -0.6465, -1.0022, -2.4071, -0.6227, -0.5275, -3.0190, -1.2567,\n",
       "         -1.0585, -1.1712, -1.0763, -0.8249, -0.6057, -1.2927, -0.4586, -0.9112,\n",
       "         -1.1309, -0.5786, -0.5237, -1.7637, -3.1013, -0.8715, -2.7871, -0.4580,\n",
       "         -0.4631, -1.4572, -0.9977, -0.6827, -0.7884, -0.7452, -0.5222, -1.0369,\n",
       "         -1.9347, -1.1607, -0.6106, -0.8509, -0.5005, -1.5337, -1.0760, -0.6981,\n",
       "         -1.8792, -0.7825, -0.8218, -0.5747, -1.1375, -1.6738, -3.4018, -0.6207,\n",
       "         -0.5272, -0.7465, -0.6078, -2.1174, -0.7154, -0.8595, -1.2609, -0.5821,\n",
       "         -0.6965, -1.1027, -0.7532, -0.8220, -1.5879, -0.4562, -1.4674, -1.0866,\n",
       "         -1.0956, -1.1703, -0.9958, -1.6562, -0.4879, -0.9152, -1.1034, -0.6414,\n",
       "         -0.5526, -0.5332, -0.6535, -1.7251, -1.0368, -0.5491, -1.3091, -0.5326,\n",
       "         -0.4714, -0.4675, -1.6049, -0.7276, -0.5930, -1.3206, -1.1686, -0.7681,\n",
       "         -0.5542, -3.9241, -0.5681, -0.9579, -0.8449, -0.6361, -1.0463, -0.9406,\n",
       "         -1.8318, -0.7067, -1.0494, -0.4476]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_prob_part(inter_times,context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
