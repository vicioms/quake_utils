{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from libs.sequences import SeismicSequence\n",
    "from libs.iris import irisRequests\n",
    "from libs.distributions import Weibull, WeibullMM, GaussianMM, InterTimeDistribution\n",
    "from importlib import reload  # Python 3.4+\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_nll_loss(inter_times : torch.Tensor, seq_lengths : torch.Tensor, inter_time_distr : InterTimeDistribution):\n",
    "    log_prob = inter_time_distr.get_log_prob(inter_times)\n",
    "    mask = SeismicSequence.pack_sequences_mask(seq_lengths, inter_times.shape[1])\n",
    "    log_like = (log_prob * mask).sum(-1)  # (N,)\n",
    "    log_surv = inter_time_distr.get_log_survival(inter_times)  # (N, L)\n",
    "    end_idx = torch.unsqueeze(seq_lengths,-1)  # (N, 1)\n",
    "    log_surv_last = torch.gather(log_surv, dim=-1, index=end_idx)  # (N, 1)\n",
    "    log_like += log_surv_last.squeeze(-1)  # (N,)\n",
    "    return -log_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUPointProcess(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_mixtures=1):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(input_dim,hidden_dim,\n",
    "                          num_layers=1,\n",
    "                          batch_first=True)\n",
    "        self.n_mixtures = n_mixtures\n",
    "        if(self.n_mixtures==1):\n",
    "            self.weibull_mod = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, 2),\n",
    "                nn.Softplus()\n",
    "            )\n",
    "        else:\n",
    "            self.weibull_mod = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, self.n_mixtures*2),\n",
    "                nn.Softplus()\n",
    "            )\n",
    "            self.mixture_mod = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, self.n_mixtures),\n",
    "                nn.Softmax(dim=-1)\n",
    "            )\n",
    "    def forward(self,x):\n",
    "        rnn_output, _ = self.rnn(x)\n",
    "        # shift forward along the time dimension and pad\n",
    "        # so we can use it to model the inter times\n",
    "        context = F.pad(rnn_output[:, :-1, :], (0,0, 1,0))\n",
    "        weibull_params = self.weibull_mod(context)\n",
    "        if(self.n_mixtures == 1):\n",
    "            distr = Weibull(weibull_params[..., 0], weibull_params[..., 1])\n",
    "        else:\n",
    "            pip_params = self.mixture_mod(context)\n",
    "            distr = WeibullMM(pip_params, weibull_params[..., :self.n_mixtures], weibull_params[..., self.n_mixtures:])\n",
    "        return context, distr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify regions limits\n",
    "regions = {}\n",
    "regions['greece'] = (30, 45,18, 44)\n",
    "regions['california'] = (30, 41, -125, -113)\n",
    "regions['japan'] = (20, 50, 120, 150)\n",
    "regions['italy'] = (35,46,6, 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_cat = True\n",
    "region_name = \"japan\"\n",
    "region = regions[region_name]\n",
    "if(load_cat):\n",
    "    df = pd.read_csv(\"catalogs/\" + region_name + \".csv\", parse_dates=['Time'])\n",
    "else:\n",
    "    start_time = datetime.datetime(1980, 1, 1, 0, 0, 0)\n",
    "    end_time =  datetime.datetime(2024, 1, 1, 0, 0, 0)\n",
    "    download_url =irisRequests.url_events_box(start_time, end_time, region[0], region[1], region[2], region[3], minmag=3, magtype=\"MW\")\n",
    "    df = pd.read_csv(download_url, sep=\"|\", comment=\"#\")\n",
    "    df.Time = pd.to_datetime(df.Time, errors='coerce')\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "    df.sort_values(by=\"Time\", inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df.to_csv(\"catalogs/japan.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = SeismicSequence.from_pandas_df(df[df.Time < datetime.datetime(2012, 1, 1, 0, 0, 0, tzinfo=datetime.UTC)], unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GRUPointProcess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/schimmenti/Desktop/quake_utils/test.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/schimmenti/Desktop/quake_utils/test.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m GRUPointProcess(\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m, n_mixtures\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GRUPointProcess' is not defined"
     ]
    }
   ],
   "source": [
    "model = GRUPointProcess(1, 10, n_mixtures=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "use_random_length = True\n",
    "duration_days = 3\n",
    "duration = 60*60*24*duration_days # in seconds\n",
    "for epoch in range(epochs):\n",
    "    print(epoch)\n",
    "    with torch.no_grad():\n",
    "        seqs = []\n",
    "        t_start = duration\n",
    "        max_t_end = train_sequences.t_end\n",
    "        while(True):\n",
    "            if(use_random_length):\n",
    "                t_end = t_start + np.random.exponential()*duration\n",
    "            else:\n",
    "                t_end = t_start + duration\n",
    "            if(t_end > max_t_end):\n",
    "                break\n",
    "            sub_seq = train_sequences.get_subsequence(t_start, t_end)\n",
    "            if(len(sub_seq.arrival_times) > 0):\n",
    "                seqs.append(sub_seq)\n",
    "            t_start = t_end\n",
    "        inter_times, features, lengths = SeismicSequence.pack_sequences(seqs) \n",
    "    optimizer.zero_grad()\n",
    "    input_time_features = inter_times.unsqueeze(-1)\n",
    "    context, distr = model(input_time_features)\n",
    "    loss = get_time_nll_loss(inter_times, lengths, distr).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
