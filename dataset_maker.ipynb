{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from iris import irisRequests\n",
    "import ngl\n",
    "import datetime\n",
    "from numba import njit\n",
    "from numba.typed import Dict\n",
    "from sklearn.metrics.pairwise import haversine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def construct_aftershocks_map_steps(region, coords, dlat, dlon, steps):\n",
    "    N_lat = int((region[1]-region[0])/dlat)\n",
    "    N_lon = int((region[3]-region[2])/dlon)\n",
    "    shocks_map = np.zeros((N_lat, N_lon))\n",
    "    for coord in coords:\n",
    "        i,j = int((coord[0]-region[0])/dlat), int((coord[1]-region[2])/dlon)\n",
    "        if(steps > 0):\n",
    "            for di in range(-steps,steps+1):\n",
    "                for dj in range(-steps, steps+1):\n",
    "                    if(i+di >= 0 and i+di < N_lat and j+dj >= 0 and j+dj < N_lon):\n",
    "                        shocks_map[i+di,j+dj] += 1\n",
    "            #for di in range(-steps,0):\n",
    "            #    if(i+di >= 0):\n",
    "            #        shocks_map[i+di,j] += 1\n",
    "            #for di in range(1,steps+1):\n",
    "            #    if(i+di < N_lat):\n",
    "            #        shocks_map[i+di,j] += 1\n",
    "            #for dj in range(-steps,0):\n",
    "            #    if(j+dj >= 0):\n",
    "            #        shocks_map[i,j+dj] += 1\n",
    "            #for dj in range(1,steps+1):\n",
    "            #    if(j+dj < N_lon):\n",
    "            #        shocks_map[i,j+dj] += 1\n",
    "        else:\n",
    "            shocks_map[i,j] += 1\n",
    "    return shocks_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def construct_aftershocks_map(region, coords, dlat, dlon):\n",
    "    N_lat = int((region[1]-region[0])/dlat)\n",
    "    N_lon = int((region[3]-region[2])/dlon)\n",
    "    shocks_map = np.zeros((N_lat, N_lon))\n",
    "    for coord in coords:\n",
    "        i,j = int((coord[0]-region[0])/dlat), int((coord[1]-region[2])/dlon)\n",
    "        shocks_map[i,j] += 1\n",
    "        #if(i>0):\n",
    "        #    shocks_map[i-1,j] += 1\n",
    "        #if(j>0):\n",
    "        #    shocks_map[i,j-1] += 1\n",
    "        #if(i < N_lat-1):\n",
    "        #    shocks_map[i+1,j] += 1\n",
    "        #if(j < N_lon-1):\n",
    "        #    shocks_map[i,j+1] += 1\n",
    "    return shocks_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath = \"csv_24/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = {}\n",
    "regions['greece'] = (30, 45,18, 44)\n",
    "regions['california'] = (30, 41, -125, -113)\n",
    "regions['japan'] = (20, 50, 120, 150)\n",
    "regions['italy'] = (35,46,6, 19)\n",
    "regions_m = {}\n",
    "regions_m['japan'] = (6.5, 4.0)\n",
    "regions_m['italy'] = (5.0, 3.0)\n",
    "regions_m['greece'] = (6.0, 4.0)\n",
    "regions_m['california'] = (5.0, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_list = ngl.ngl_process_list(ngl.ngl_24h_2w) # daily measurements, with 2 weeks delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_info = {}\n",
    "for name, region in regions.items():\n",
    "    station_names, station_lats, station_lons  = ngl.get_all_stations_box(station_list, *region)\n",
    "    station_info[name]= (station_names, station_lats, station_lons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABEL downloaded\n",
      "ADAN downloaded\n",
      "ADCS downloaded\n",
      "ADIY downloaded\n",
      "ADN1 downloaded\n",
      "ADN2 downloaded\n",
      "ADY1 downloaded\n",
      "AFY0 downloaded\n",
      "AFYN downloaded\n",
      "AFYT downloaded\n",
      "AGID downloaded\n",
      "AGIG downloaded\n",
      "AGIO downloaded\n",
      "AGNA downloaded\n",
      "AGRI downloaded\n",
      "AIGI downloaded\n",
      "AKD1 downloaded\n",
      "AKDG downloaded\n",
      "AKHI downloaded\n",
      "AKHR downloaded\n",
      "AKHU downloaded\n",
      "AKLE downloaded\n",
      "AKS1 downloaded\n",
      "AKSR downloaded\n",
      "AKYR downloaded\n",
      "ALE3 downloaded\n",
      "ALNY downloaded\n",
      "ALON downloaded\n",
      "ALX2 downloaded\n",
      "ALXR downloaded\n",
      "AMA3 downloaded\n",
      "AMAS downloaded\n",
      "AMFI downloaded\n",
      "AMMN downloaded\n",
      "ANAV downloaded\n",
      "ANDR downloaded\n",
      "ANIK downloaded\n",
      "ANK2 downloaded\n",
      "ANKR downloaded\n",
      "ANKY downloaded\n",
      "ANMU downloaded\n",
      "ANOC downloaded\n",
      "ANOP downloaded\n",
      "ANRK downloaded\n",
      "ANTA downloaded\n",
      "ANTL downloaded\n",
      "ANTP downloaded\n",
      "APK1 downloaded\n",
      "ARDA downloaded\n",
      "AREL downloaded\n",
      "ARG2 downloaded\n",
      "ARK1 downloaded\n",
      "ARKI downloaded\n",
      "ARPK downloaded\n",
      "ARSA downloaded\n",
      "ARST downloaded\n",
      "ART1 downloaded\n",
      "ART2 downloaded\n",
      "ARTV downloaded\n",
      "ASGA downloaded\n",
      "ASSO downloaded\n",
      "AST5 downloaded\n",
      "ASTY downloaded\n",
      "ATAL downloaded\n",
      "ATER downloaded\n",
      "ATHI downloaded\n",
      "ATRS downloaded\n",
      "AUT1 downloaded\n",
      "AYD1 downloaded\n",
      "AYD2 downloaded\n",
      "AYVL downloaded\n",
      "BABA downloaded\n",
      "BACU downloaded\n",
      "BAES downloaded\n",
      "BAIS downloaded\n",
      "BAJR downloaded\n",
      "BAL1 downloaded\n",
      "BALJ downloaded\n",
      "BALK downloaded\n",
      "BAND downloaded\n",
      "BANO loaded\n",
      "BAYB downloaded\n",
      "BCAK downloaded\n",
      "BEL2 downloaded\n",
      "BER2 downloaded\n",
      "BERA downloaded\n",
      "BERO downloaded\n",
      "BEYS downloaded\n",
      "BH02 loaded\n",
      "BILE downloaded\n",
      "BIN2 downloaded\n",
      "BIN4 downloaded\n",
      "BITO downloaded\n",
      "BM51 downloaded\n",
      "BNDR downloaded\n",
      "BOG1 downloaded\n",
      "BOGZ downloaded\n",
      "BOL2 downloaded\n",
      "BOLU downloaded\n",
      "BOTE downloaded\n",
      "BOYT downloaded\n",
      "BOZU downloaded\n",
      "BSHM downloaded\n",
      "BTMN downloaded\n",
      "BUCA downloaded\n",
      "BUCU downloaded\n",
      "BUR3 downloaded\n",
      "BURS downloaded\n",
      "BYB1 downloaded\n",
      "CALR downloaded\n",
      "CAN1 downloaded\n",
      "CANA downloaded\n",
      "CANK downloaded\n",
      "CAT9 downloaded\n",
      "CAV1 downloaded\n",
      "CAV2 downloaded\n",
      "CAVD downloaded\n",
      "CES3 downloaded\n",
      "CESM downloaded\n",
      "CHIU downloaded\n",
      "CIHA downloaded\n",
      "CMLD downloaded\n",
      "CONA downloaded\n",
      "CONB downloaded\n",
      "CORA downloaded\n",
      "CORU downloaded\n",
      "COST downloaded\n",
      "COTI downloaded\n",
      "CRAI downloaded\n",
      "CRAO downloaded\n",
      "CRNV downloaded\n",
      "CRUM downloaded\n",
      "CSAR downloaded\n",
      "CSSR loaded\n",
      "CYL2 downloaded\n",
      "CZRE downloaded\n",
      "DATC downloaded\n",
      "DEBA downloaded\n",
      "DEI1 downloaded\n",
      "DEIR downloaded\n",
      "DES1 downloaded\n",
      "DES2 downloaded\n",
      "DES3 downloaded\n",
      "DIDI downloaded\n",
      "DIMI downloaded\n",
      "DINA downloaded\n",
      "DION downloaded\n",
      "DIPK downloaded\n",
      "DIV2 downloaded\n",
      "DIV3 downloaded\n",
      "DIVR downloaded\n",
      "DIY1 downloaded\n",
      "DIYB downloaded\n",
      "DMDM downloaded\n",
      "DNZ1 downloaded\n",
      "DNZL downloaded\n",
      "DPK1 downloaded\n",
      "DRA1 downloaded\n",
      "DRA9 downloaded\n",
      "DRAG downloaded\n",
      "DRAM downloaded\n",
      "DRAN downloaded\n",
      "DRTS downloaded\n",
      "DSEA downloaded\n",
      "DSLN downloaded\n",
      "DUB2 loaded\n",
      "DUBR loaded\n",
      "DUR2 downloaded\n",
      "DUTH downloaded\n",
      "DYAR downloaded\n",
      "DYNG downloaded\n",
      "DYR2 downloaded\n",
      "EDES downloaded\n",
      "EDIR downloaded\n",
      "EGIO downloaded\n",
      "EKIZ downloaded\n",
      "EKZ1 downloaded\n",
      "ELAZ downloaded\n",
      "ELMI downloaded\n",
      "ELRO downloaded\n",
      "EMIR downloaded\n",
      "EMR2 downloaded\n",
      "ERGN downloaded\n",
      "ERZ1 downloaded\n",
      "ERZ2 downloaded\n",
      "ERZI downloaded\n",
      "ERZR downloaded\n",
      "ESKS downloaded\n",
      "EXAN downloaded\n",
      "EYPA downloaded\n",
      "FARS downloaded\n",
      "FAS2 downloaded\n",
      "FEEK downloaded\n",
      "FETH downloaded\n",
      "FINI downloaded\n",
      "FISK downloaded\n",
      "FLO2 downloaded\n",
      "FOCA loaded\n",
      "GAB1 downloaded\n",
      "GAL3 downloaded\n",
      "GEM1 downloaded\n",
      "GEME downloaded\n",
      "GEYB downloaded\n",
      "GHAJ downloaded\n",
      "GILB downloaded\n",
      "GIRS downloaded\n",
      "GIUG downloaded\n",
      "GIUR loaded\n",
      "GJML downloaded\n",
      "GOD1 downloaded\n",
      "GODA downloaded\n",
      "GON1 downloaded\n",
      "GOUM downloaded\n",
      "GPOR downloaded\n",
      "GRA4 downloaded\n",
      "GRAM downloaded\n",
      "GRDC loaded\n",
      "GREV downloaded\n",
      "GUMU downloaded\n",
      "GURU downloaded\n",
      "GVDS downloaded\n",
      "GYUR downloaded\n",
      "HAK1 downloaded\n",
      "HAKK downloaded\n",
      "HALK downloaded\n",
      "HALP downloaded\n",
      "HAMA downloaded\n",
      "HAR1 downloaded\n",
      "HAR3 downloaded\n",
      "HARC downloaded\n",
      "HARM downloaded\n",
      "HAT1 downloaded\n",
      "HAT2 downloaded\n",
      "HATA downloaded\n",
      "HEND downloaded\n",
      "HERA downloaded\n",
      "HIMA downloaded\n",
      "HINI downloaded\n",
      "HIST downloaded\n",
      "HNDK downloaded\n",
      "HRAK downloaded\n",
      "HRMN downloaded\n",
      "HRR2 downloaded\n",
      "HRRN downloaded\n",
      "HSON downloaded\n",
      "HUGS downloaded\n",
      "HYM1 downloaded\n",
      "HYMN downloaded\n",
      "IDI0 downloaded\n",
      "IERA downloaded\n",
      "IGOU downloaded\n",
      "IKAR downloaded\n",
      "INE1 downloaded\n",
      "INE3 downloaded\n",
      "INEB downloaded\n",
      "INSU downloaded\n",
      "IOAN downloaded\n",
      "IPS1 downloaded\n",
      "IPS4 downloaded\n",
      "IPSA downloaded\n",
      "ISP1 downloaded\n",
      "ISPT downloaded\n",
      "ISPX downloaded\n",
      "ISSD downloaded\n",
      "ISTA downloaded\n",
      "ISTI downloaded\n",
      "ISTN downloaded\n",
      "ISTR downloaded\n",
      "ITEA downloaded\n",
      "IZAD downloaded\n",
      "IZMI downloaded\n",
      "IZMT downloaded\n",
      "IZQW downloaded\n",
      "JGC1 downloaded\n",
      "JGC4 downloaded\n",
      "JPA1 downloaded\n",
      "JSLM downloaded\n",
      "JUST downloaded\n",
      "KAAS downloaded\n",
      "KABR downloaded\n",
      "KAHR downloaded\n",
      "KAL1 downloaded\n",
      "KAL2 downloaded\n",
      "KAL3 downloaded\n",
      "KALA downloaded\n",
      "KALM downloaded\n",
      "KALY downloaded\n",
      "KAM3 downloaded\n",
      "KAMN downloaded\n",
      "KAP1 downloaded\n",
      "KAPN downloaded\n",
      "KAPS downloaded\n",
      "KAR1 downloaded\n",
      "KARB downloaded\n",
      "KARD downloaded\n",
      "KARP downloaded\n",
      "KASI downloaded\n",
      "KAST downloaded\n",
      "KATC downloaded\n",
      "KATE downloaded\n",
      "KATZ downloaded\n",
      "KAV1 downloaded\n",
      "KAVA downloaded\n",
      "KAY1 downloaded\n",
      "KAYS downloaded\n",
      "KCAP downloaded\n",
      "KER1 downloaded\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/apacco/Documents/GIT_works/quake_utils/dataset_maker.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/apacco/Documents/GIT_works/quake_utils/dataset_maker.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m name, s_info \u001b[39min\u001b[39;00m station_info\u001b[39m.\u001b[39mitems():\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/apacco/Documents/GIT_works/quake_utils/dataset_maker.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mfor\u001b[39;00m s_cnt, s_name \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(s_info[\u001b[39m0\u001b[39m]):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/apacco/Documents/GIT_works/quake_utils/dataset_maker.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         df, status \u001b[39m=\u001b[39m ngl\u001b[39m.\u001b[39;49mngl_retrieve_24h(rootpath, s_name)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/apacco/Documents/GIT_works/quake_utils/dataset_maker.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39mprint\u001b[39m(s_name, status)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/apacco/Documents/GIT_works/quake_utils/dataset_maker.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         station_data[s_name] \u001b[39m=\u001b[39m df\n",
      "File \u001b[0;32m~/Documents/GIT_works/quake_utils/ngl.py:115\u001b[0m, in \u001b[0;36mngl_retrieve_24h\u001b[0;34m(rootpath, station_name, force_download)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[39mreturn\u001b[39;00m data, \u001b[39m\"\u001b[39m\u001b[39mloaded\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m download_url \u001b[39m=\u001b[39m base_url \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/IGS14/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m station_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m data_type\n\u001b[0;32m--> 115\u001b[0m data \u001b[39m=\u001b[39m  pd\u001b[39m.\u001b[39;49mread_csv(download_url, sep\u001b[39m=\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39ms+\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    116\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [str_to_datetime_2000(\u001b[39mstr\u001b[39m(s)) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m data[\u001b[39m'\u001b[39m\u001b[39mYYMMMDD\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m    117\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mdatetime64[D]\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/common.py:713\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    710\u001b[0m     codecs\u001b[39m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    712\u001b[0m \u001b[39m# open URLs\u001b[39;00m\n\u001b[0;32m--> 713\u001b[0m ioargs \u001b[39m=\u001b[39m _get_filepath_or_buffer(\n\u001b[1;32m    714\u001b[0m     path_or_buf,\n\u001b[1;32m    715\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    716\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    717\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    718\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    719\u001b[0m )\n\u001b[1;32m    721\u001b[0m handle \u001b[39m=\u001b[39m ioargs\u001b[39m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    722\u001b[0m handles: \u001b[39mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/common.py:368\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[39mif\u001b[39;00m content_encoding \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    366\u001b[0m             \u001b[39m# Override compression based on Content-Encoding header\u001b[39;00m\n\u001b[1;32m    367\u001b[0m             compression \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m--> 368\u001b[0m         reader \u001b[39m=\u001b[39m BytesIO(req\u001b[39m.\u001b[39;49mread())\n\u001b[1;32m    369\u001b[0m     \u001b[39mreturn\u001b[39;00m IOArgs(\n\u001b[1;32m    370\u001b[0m         filepath_or_buffer\u001b[39m=\u001b[39mreader,\n\u001b[1;32m    371\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m         mode\u001b[39m=\u001b[39mfsspec_mode,\n\u001b[1;32m    375\u001b[0m     )\n\u001b[1;32m    377\u001b[0m \u001b[39mif\u001b[39;00m is_fsspec_url(filepath_or_buffer):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:481\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 481\u001b[0m         s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_safe_read(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlength)\n\u001b[1;32m    482\u001b[0m     \u001b[39mexcept\u001b[39;00m IncompleteRead:\n\u001b[1;32m    483\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:630\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_safe_read\u001b[39m(\u001b[39mself\u001b[39m, amt):\n\u001b[1;32m    624\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \n\u001b[1;32m    626\u001b[0m \u001b[39m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[39m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[39m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mread(amt)\n\u001b[1;32m    631\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m<\u001b[39m amt:\n\u001b[1;32m    632\u001b[0m         \u001b[39mraise\u001b[39;00m IncompleteRead(data, amt\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(data))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "station_data = {}\n",
    "for name, s_info in station_info.items():\n",
    "    for s_cnt, s_name in enumerate(s_info[0]):\n",
    "        df, status = ngl.ngl_retrieve_24h(rootpath, s_name)\n",
    "        print(s_name, status)\n",
    "        station_data[s_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime(2003, 1, 1, 0, 0, 0)\n",
    "end_time =  datetime.datetime(2024, 1, 1, 0, 0, 0)\n",
    "catalogs = {}\n",
    "for name, region in regions.items():\n",
    "    download_url =irisRequests.url_events_box(start_time, end_time, region[0], region[1], region[2], region[3], minmag=3, magtype=\"MW\")\n",
    "    df = pd.read_csv(download_url, sep=\"|\", comment=\"#\")\n",
    "    df.Time = pd.to_datetime(df.Time, errors='coerce')\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "    df.sort_values(by=\"Time\", inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    catalogs[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(nogil=True)\n",
    "def construct_map(u, idx, d, d_cutoff = 0.01):\n",
    "    # u = (N_s, 3)\n",
    "    # idx = (N_s)\n",
    "    # d = (N_lat, N_lon, N_s_tot)\n",
    "    u_map = np.zeros((d.shape[0], d.shape[1], 3))\n",
    "    for i in range(0, d.shape[0]):\n",
    "        for j in range(0, d.shape[1]):\n",
    "            cnt = 0\n",
    "            for i_n,n in enumerate(idx):\n",
    "                d_ijn = d[i,j,n]\n",
    "                if(d_ijn <= d_cutoff):\n",
    "                    u_map[i,j,:] = u_map[i,j,:] + u[i_n, :]\n",
    "                    cnt += 1\n",
    "            if(cnt > 0):\n",
    "                u_map[i,j,:] = u_map[i,j,:]/float(cnt)\n",
    "            else:\n",
    "                u_map[i,j,0] = np.nan\n",
    "                u_map[i,j,1] = np.nan\n",
    "                u_map[i,j,2] = np.nan\n",
    "    return u_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximal_time_shift = np.timedelta64(7, 'D')\n",
    "minimal_time_shift = np.timedelta64(1, 'D')\n",
    "dlat = 0.1\n",
    "dlon = 0.1\n",
    "datasets = {}\n",
    "for name, catalog in catalogs.items():\n",
    "    dataset = []\n",
    "    print(name)\n",
    "    # discretization of the region\n",
    "    region = regions[name]\n",
    "    N_lat = int((region[1]-region[0])/dlat)\n",
    "    N_lon = int((region[3]-region[2])/dlon)\n",
    "    grid_latlat, grid_lonlon = np.meshgrid( region[0] + np.arange(0, N_lat)*dlat, region[2] + np.arange(0, N_lon)*dlon, indexing='ij')\n",
    "    grid_latlat = grid_latlat.flatten()\n",
    "    grid_lonlon = grid_lonlon.flatten()\n",
    "    grid = np.hstack([grid_latlat[:,None], grid_lonlon[:,None]])\n",
    "    stations_coords = np.hstack([station_info[name][1][:,None], station_info[name][2][:,None]])\n",
    "    grid_stations_dists = haversine_distances(np.radians(grid), np.radians(stations_coords))\n",
    "    grid_stations_dists = grid_stations_dists.reshape((N_lat, N_lon, -1))\n",
    "    \n",
    "\n",
    "    # earthquakes identification\n",
    "    large_shocks = catalog[catalog.Magnitude >= regions_m[name][0]]\n",
    "    large_shocks_days = large_shocks.Time\n",
    "    large_shocks_coords =  large_shocks[['Latitude','Longitude']].values\n",
    "    for day_time, coord in zip(large_shocks_days, large_shocks_coords):\n",
    "        print(day_time)\n",
    "        day = np.datetime64(datetime.datetime(day_time.year, day_time.month, day_time.day))\n",
    "        aftershocks = catalog[ (catalog.Magnitude >= regions_m[name][1])* (catalog.Time.values.astype('datetime64[D]') >=  day + minimal_time_shift)*(catalog.Time.values.astype('datetime64[D]') <= pd.to_datetime(day) +maximal_time_shift ) ]\n",
    "        aftershocks_coords = aftershocks[['Latitude','Longitude']].values\n",
    "        #aftershocks_map = construct_aftershocks_map(region, aftershocks_coords, dlat, dlon)\n",
    "        aftershocks_maps = [construct_aftershocks_map_steps(region, aftershocks_coords, dlat, dlon, steps) for steps in [0,1,2] ]\n",
    "        # daily data\n",
    "        data = []\n",
    "        indices = []\n",
    "        for cnt, (s_name, _,__) in  enumerate(zip(*station_info[name])):\n",
    "            df = station_data[s_name]\n",
    "            row = df[df.date==  day]\n",
    "            row_p = df[df.date==  day - np.timedelta64(1, 'D')]\n",
    "            if(len(row) > 0 and len(row_p) > 0):\n",
    "                u_n = row.lat.values[0]-row_p.lat.values[0]\n",
    "                u_e = row.lon.values[0]-row_p.lon.values[0]\n",
    "                u_v = row.height.values[0] - row_p.height.values[0]\n",
    "                data.append((u_n, u_e, u_v))\n",
    "                indices.append(cnt)\n",
    "        if(len(indices) > 0):\n",
    "            data = np.array(data)\n",
    "            indices = np.array(indices)\n",
    "            \n",
    "            u_map = construct_map(data, indices, grid_stations_dists)\n",
    "            dataset.append((day_time, coord,u_map, aftershocks_maps))\n",
    "            #print(data)\n",
    "            fig, ax = plt.subplots(ncols=4, figsize=(16,3))\n",
    "            ax[0].contourf(np.sqrt(u_map[:,:,0]**2+u_map[:,:,1]**2), cmap='hsv')\n",
    "            for am_c, am in enumerate(aftershocks_maps):\n",
    "                ax[am_c+1].contourf(am)\n",
    "            #locs = np.argwhere(aftershocks_map>0)\n",
    "            #ax[0].contour(aftershocks_map, cmap='Greys', levels=2)\n",
    "            #ax.scatter(locs[:,1], locs[:,0], color='black', s=5)\n",
    "            plt.show()\n",
    "            print(len(indices))\n",
    "    datasets[name] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"japan\"\n",
    "dataset = datasets[study_name]\n",
    "x_train = []\n",
    "x_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "limit_date = datetime.datetime(2015,1,1,0,0,0, tzinfo=datetime.timezone.utc)\n",
    "for data in dataset:\n",
    "    if(data[0]  > limit_date):\n",
    "        y_test.append(data[3][-2])\n",
    "        x_test.append(data[2])\n",
    "    else:\n",
    "        y_train.append(data[3][-2])\n",
    "        x_train.append(data[2])\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "np.savez(study_name + \".npz\", x_train=x_train, x_test=x_test, y_train = y_train, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
