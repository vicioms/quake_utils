{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from iris import irisRequests\n",
    "import ngl\n",
    "import datetime\n",
    "from numba import njit\n",
    "from numba.typed import Dict\n",
    "from sklearn.metrics.pairwise import haversine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def construct_aftershocks_map(region, coords, dlat, dlon):\n",
    "    N_lat = int((region[1]-region[0])/dlat)\n",
    "    N_lon = int((region[3]-region[2])/dlon)\n",
    "    shocks_map = np.zeros((N_lat, N_lon))\n",
    "    for coord in coords:\n",
    "        i,j = int((coord[0]-region[0])/dlat), int((coord[1]-region[2])/dlon)\n",
    "        shocks_map[i,j] += 1\n",
    "        #if(i>0):\n",
    "        #    shocks_map[i-1,j] += 1\n",
    "        #if(j>0):\n",
    "        #    shocks_map[i,j-1] += 1\n",
    "        #if(i < N_lat-1):\n",
    "        #    shocks_map[i+1,j] += 1\n",
    "        #if(j < N_lon-1):\n",
    "        #    shocks_map[i,j+1] += 1\n",
    "    return shocks_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath = \"csv_24/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = {}\n",
    "regions['greece'] = (30, 42,18, 44)\n",
    "regions['california'] = (30, 41, -125, -113)\n",
    "regions['japan'] = (20, 50, 120, 150)\n",
    "regions['italy'] = (35,46,6, 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_list = ngl.ngl_process_list(ngl.ngl_24h_2w) # daily measurements, with 2 weeks delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_info = {}\n",
    "for name, region in regions.items():\n",
    "    station_names, station_lats, station_lons  = ngl.get_all_stations_box(station_list, *region)\n",
    "    station_info[name]= (station_names, station_lats, station_lons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data = {}\n",
    "for name, s_info in station_info.items():\n",
    "    for s_cnt, s_name in enumerate(s_info[0]):\n",
    "        df, status = ngl.ngl_retrieve_24h(rootpath, s_name)\n",
    "        print(s_name, status)\n",
    "        station_data[s_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime(2012, 1, 1, 0, 0, 0)\n",
    "end_time =  datetime.datetime(2024, 1, 1, 0, 0, 0)\n",
    "catalogs = {}\n",
    "for name, region in regions.items():\n",
    "    download_url =irisRequests.url_events_box(start_time, end_time, region[0], region[1], region[2], region[3], minmag=3, magtype=\"MW\")\n",
    "    df = pd.read_csv(download_url, sep=\"|\", comment=\"#\", parse_dates=[\"Time\"])\n",
    "    df.sort_values(by=\"Time\", inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    catalogs[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(nogil=True)\n",
    "def construct_map(u, idx, d, d_cutoff = 0.01):\n",
    "    # u = (N_s, 3)\n",
    "    # idx = (N_s)\n",
    "    # d = (N_lat, N_lon, N_s_tot)\n",
    "    u_map = np.zeros((d.shape[0], d.shape[1], 3))\n",
    "    for i in range(0, d.shape[0]):\n",
    "        for j in range(0, d.shape[1]):\n",
    "            cnt = 0\n",
    "            for i_n,n in enumerate(idx):\n",
    "                d_ijn = d[i,j,n]\n",
    "                if(d_ijn <= d_cutoff):\n",
    "                    u_map[i,j,:] = u_map[i,j,:] + u[i_n, :]\n",
    "                    cnt += 1\n",
    "            if(cnt > 0):\n",
    "                u_map[i,j,:] = u_map[i,j,:]/float(cnt)\n",
    "            else:\n",
    "                u_map[i,j,0] = np.nan\n",
    "                u_map[i,j,1] = np.nan\n",
    "                u_map[i,j,2] = np.nan\n",
    "    return u_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximal_time_shift = np.timedelta64(7, 'D')\n",
    "minimal_time_shift = np.timedelta64(1, 'D')\n",
    "dlat = 0.1\n",
    "dlon = 0.1\n",
    "datasets = {}\n",
    "for name, catalog in catalogs.items():\n",
    "    dataset = []\n",
    "    print(name)\n",
    "    # discretization of the region\n",
    "    region = regions[name]\n",
    "    N_lat = int((region[1]-region[0])/dlat)\n",
    "    N_lon = int((region[3]-region[2])/dlon)\n",
    "    grid_latlat, grid_lonlon = np.meshgrid( region[0] + np.arange(0, N_lat)*dlat, region[2] + np.arange(0, N_lon)*dlon, indexing='ij')\n",
    "    grid_latlat = grid_latlat.flatten()\n",
    "    grid_lonlon = grid_lonlon.flatten()\n",
    "    grid = np.hstack([grid_latlat[:,None], grid_lonlon[:,None]])\n",
    "    stations_coords = np.hstack([station_info[name][1][:,None], station_info[name][2][:,None]])\n",
    "    grid_stations_dists = haversine_distances(np.radians(grid), np.radians(stations_coords))\n",
    "    grid_stations_dists = grid_stations_dists.reshape((N_lat, N_lon, -1))\n",
    "    \n",
    "\n",
    "    # earthquakes identification\n",
    "    large_shocks = catalog[catalog.Magnitude >= 5.8]\n",
    "    large_shocks_days = large_shocks.Time\n",
    "    large_shocks_coords =  large_shocks[['Latitude','Longitude']].values\n",
    "    for day_time, coord in zip(large_shocks_days, large_shocks_coords):\n",
    "        print(day_time)\n",
    "        day = np.datetime64(datetime.datetime(day_time.year, day_time.month, day_time.day))\n",
    "        aftershocks = catalog[(catalog.Time.values.astype('datetime64[D]') >=  day + minimal_time_shift)*(catalog.Time.values.astype('datetime64[D]') <= pd.to_datetime(day) +maximal_time_shift ) ]\n",
    "        aftershocks_coords = aftershocks[['Latitude','Longitude']].values\n",
    "        aftershocks_map = construct_aftershocks_map(region, aftershocks_coords, dlat, dlon)\n",
    "        # daily data\n",
    "        data = []\n",
    "        indices = []\n",
    "        for cnt, (s_name, _,__) in  enumerate(zip(*station_info[name])):\n",
    "            df = station_data[s_name]\n",
    "            row = df[df.date==  day]\n",
    "            row_p = df[df.date==  day - np.timedelta64(1, 'D')]\n",
    "            if(len(row) > 0 and len(row_p) > 0):\n",
    "                u_n = row.lat.values[0]-row_p.lat.values[0]\n",
    "                u_e = row.lon.values[0]-row_p.lon.values[0]\n",
    "                u_v = row.height.values[0] - row_p.height.values[0]\n",
    "                data.append((u_n, u_e, u_v))\n",
    "                indices.append(cnt)\n",
    "        if(len(indices) > 0):\n",
    "            data = np.array(data)\n",
    "            indices = np.array(indices)\n",
    "            \n",
    "            u_map = construct_map(data, indices, grid_stations_dists)\n",
    "            dataset.append((day_time, coord,u_map, aftershocks_map))\n",
    "            #print(data)\n",
    "            fig, ax = plt.subplots(ncols=1, figsize=(3,3))\n",
    "            ax.contourf(np.sqrt(u_map[:,:,0]**2+u_map[:,:,1]**2), cmap='hsv')\n",
    "            locs = np.argwhere(aftershocks_map>0)\n",
    "            #ax[0].contour(aftershocks_map, cmap='Greys', levels=2)\n",
    "            ax.scatter(locs[:,1], locs[:,0], color='black', s=5)\n",
    "            plt.show()\n",
    "            print(len(indices))\n",
    "    datasets[name] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"japan\"\n",
    "dataset = datasets[study_name]\n",
    "x_train = []\n",
    "x_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "limit_date = datetime.datetime(2020,1,1,0,0,0, tzinfo=datetime.timezone.utc)\n",
    "for data in dataset:\n",
    "    if(data[0]  > limit_date):\n",
    "        y_test.append(data[3])\n",
    "        x_test.append(data[2])\n",
    "    else:\n",
    "        y_train.append(data[3])\n",
    "        x_train.append(data[2])\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "np.savez(study_name + \".npz\", x_train=x_train, x_test=x_test, y_train = y_train, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
